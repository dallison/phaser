# Phaser User Guide

Phaser is a Google Protobuf compiler plugin that provides zero-copy protobuf messages
without the need for serialization.  This means that you can create and access protobuf
IDL messages directly in a memory buffer instead of creating objects from the heap
or from an arena.  Once created, messages may be transferred to another computer,
or sent over an IPC system without any further processing.

Protobuf's version compatibility is fully supported allowing you to use different
versions of messages interchangeably.  This allows you to store the binary messages
and later retrieve them with newer software, or to communicate with computers running
different software versions.

The wire-format for Phaser messages is not the same as serialized protobuf format,
but for compatibility with protobuf, Phaser supports full protobuf serialization.  This
is useful when storing messages in a format that is recognized by services such as
BigQuery where protobuf wire-format is required.

Other features include:

1. Target IDL is proto3, but proto2 is also supported
2. Message printing to std::ostream
3. Fixed and variable-sized buffers can be used
4. User-supplied metadata can be added to the message.
5. Full support for *google.protobuf.Any*
6. Enum printing and parsing
7. Message reflection
8. Presence masks
9. Bazel build
10. Modern C++17 with Google Abseil

## How Phaser works
Phaser works as a plugin for the Protocol Buffers compiler (protoc).  This means that protoc
does the parsing of the .proto files and hands off to Phaser to generate code.

To the programmer, the messages generated by Phaser look just like those generated by
the regular C++ protobuf backend, with a few enhancements and minor incompatibilities.  They
are generated as C++ classes with accessor functions for each field - with the same names
as those generated by protobuf. 

However, while protobuf creates a tree of allocated objects holding the message data, Phaser's
message classes do not store the actual field values directly, but rather the values are
stored in a separate buffer, directly in the wire-format.  The *source* messages (the C++
classes that the programmer sees) merely store information about where the actual data
is stored in the *binary* message held in the buffer.

So, when you set a field using the *set_* function, the data is placed in the *binary*
buffer using information held in the *source* class.  This makes the source classes very
lightweight, containing just metadata about the binary message.

When you read a field from a source class, the data is read from the binary message.  However,
this is not done directly since the binary message may be a different version from that
doing the reading (fields may be been added or changed).  Therefore, reading a field results
in the use of *field metadata* that is held in a small array inside the binary message.  This
allows the software to use newer or older version of messages, one of the mainstays of
protobuf's popularity.

## What's wrong with serialization?
It all depends on what you doing with the messages and what the messages look like.  For small
workloads it is perfectly fine to use the protobuf serialization method as the time taken
to perform the conversion to and from wire-format is negligible.  However, in some performance
critical applications (like robots or AVs), messages tend to contain large sensor data streams
and the serialization time can dominate the CPU, sometimes using up to 60% of the time.  A few years
ago, Google reckoned that serialization took 30% of the CPU time in their data centers.

If you can avoid serialization, at least for the messages that matter (sensor data or other large
messages) you can save all that CPU and get much better usage out of your embedded system running
your robot.

Also, most of the time, inside a robot, you are not using different versions of software
and therefore, the version compatibility provided by serialization is not needed.  This is
especially true when you use shared memory for Interprocess Communication (IPC) where processes
can access the same physical memory directly.  Serializing it into shared memory, just to deserialize
it again in the subscribers is just a waste of valuable compute cycles.

## Zero-copy FTW
All programs that use protobuf operate in the same way: you create your messages in objects
allocated from the heap (or from an arena if you need it to go slightly faster), then you
serialize them into a buffer and send that buffer onward.  Creating your messages involves
calling setter functions to set the values.  Adding values to repeated fields involved pushing
them into a vector.  Creating message-valued fields involves allocating a new object and
setting its values.

This is all good, and serializing small messages is no biggie, but what if your message
contains a large repeated field of floating point numbers (think of a LIDAR scan) or a large
buffer (bytes field in protobuf parlance) containing an RGB camera image?  Your program will
spend a long time copying the values into place, then it will copy them again during your
serialization step.

With zero-copy, you have a facility that allows you to directly access the final location
for the data.  If you change your algorithm to take advantage of this direct access, you
can speed up the message creation by up to an order of magnitude.  So instead of
looping over a set of floating point values, adding each to a vector, you can get a pointer
to the start of the vector's memory and write directly to it.

Another common message pattern is a repeated field of messages.  In this case you create
a new object for each element, add it to the vector and fill them in.  With zero-copy
you can add a bunch of messages at once, performing only one allocation and then
fill all of them in.  A huge saving.

